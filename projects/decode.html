<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>DeCoDe</title>
  <link rel="stylesheet" href="/style.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha512-..." crossorigin="anonymous" referrerpolicy="no-referrer"/>
</head>

<body>
  <div id="navbar"></div>
  <script>
    fetch('/navbar.html')
      .then(res => res.text())
      .then(html => {
        document.getElementById('navbar').innerHTML = html;
      });
  </script>
  </div>

    <div  class="container">
      <main class="main-content">
        <h1> DeCoDe

            <div class="social-icons", style="display:flex; justify-content:flex-start">
                <a href="https://github.com/anniedoris/design_qa" target="_blank" aria-label="GitHub">
                <i class="fab fa-github"></i>
                </a>

                <a href="https://design-qa.github.io/" target="_blank" aria-label="GitHub">
                <i class="fas fa-globe"></i>
                </a>

                <a href="https://asmedigitalcollection.asme.org/computingengineering/article/25/2/021009/1210215/DesignQA-A-Multimodal-Benchmark-for-Evaluating" target="_blank" aria-label="GitHub">
                <i class="fas fa-file-alt"></i>
                </a>
            </div>
        </h1>

        <h2 style="text-align:center; margin-left: 10rem; margin-right: 10rem;"> DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation </h2>

        <figure style="text-align:center;">
        <img src="/assets/DeCoDe Small.png"
            style="display:block;margin:0 auto;width:40%;max-width:800px;height:auto;">
        <figcaption> Cad of MY24, the MIT Motorsports performance race car.</figcaption>
        </figure>

        <p>
          This research introduces DesignQA, a novel benchmark aimed at evaluating the proficiency of multimodal large language models (MLLMs) in comprehending and applying engineering requirements in technical documentation. Developed with a focus on real-world engineering challenges, DesignQA uniquely combines multimodal data—including textual design requirements, CAD images, and engineering drawings—derived from the Formula SAE student competition. Unlike many existing MLLM benchmarks, DesignQA contains document-grounded visual questions where the input image and the input document come from different sources. The benchmark features automatic evaluation metrics and is divided into segments—Rule Comprehension, Rule Compliance, and Rule Extraction—based on tasks that engineers perform when designing according to requirements. We evaluate state-of-the-art models (at the time of writing) like GPT-4o, GPT-4, Claude-Opus, Gemini-1.0, and LLaVA-1.5 against the benchmark, and our study uncovers the existing gaps in MLLMs’ abilities to interpret complex engineering documentation. The MLLMs tested, while promising, struggle to reliably retrieve relevant rules from the Formula SAE documentation, face challenges in recognizing technical components in CAD images and encounter difficulty in analyzing engineering drawings. These findings underscore the need for multimodal models that can better handle the multifaceted questions characteristic of design according to technical documentation. This benchmark sets a foundation for future advancements in AI-supported engineering design processes. DesignQA is publicly available at online.
        </p>
      </main>
    </div>

  <script src="/scripts.js"></script>
</body>
</html>
